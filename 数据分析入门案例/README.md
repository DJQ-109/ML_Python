
 详细描述见博客：
 https://zhuanlan.zhihu.com/p/95040450
 
 
 
1、功能综述
该部分可以实现将用户的csv文件进行一系列数据分析并且返回相应的分析结果。这部分主要的分析包括返回数据主要属性，相关性信息、热力图，特征分布散点图、密度图，缺省值统计和补全，统计偏值并归正，生成词云等，将这些在后台分析出来的数据、图片等返回给用户，实现一个基本的csv文件通用分析处理功能。
 
2、功能实现
定义DataAnalyze类，其中文件file作为其中的属性，定义相关函数来实现对应的功能。
2.1返回数据属性
为了来给用户对该数据有一个最直观的认识，定义check_structure函数，返回数据的形状和每列数据的属性（包括索引范围、列属性的数量，是否为空，数据类型等），该部分主要是用dataframe数据类型的index和columns属性来获得数据的行数和列数；在原来的内置函数info函数的基础上进行改进，可以返回数据每种特征的名字，每个特征有多少个值，是否是空，数据类型等。以drinks.cvs文件为例子，输出该函数的返回值如下：


数据基本属性
 
2.2特征相关性
在数据分析中，相关性分析是一个非常重要的一部分，可以由此找到数值型属性中那些属性的关系最近，即那些属性最重要，最值得花时间来研究。定义correlation函数，在里面返回各种数值型属性之间的相关性的数值，同时利用seaborn库来画出关系热力图，更加直观。可以直接借助dataframe的corr函数来获得任意两个属性的关系数据，利用sns.heatmap(df.corr(), cmap="YlGnBu")做出热力图。还是使用上面的例子，运行后显示如下：


每个数值型的特征之间相关值
 



 
颜色越深，表示两个特征之间的关系越密切，相互之间的影响更大。例如beer_servings和自己的关系最大为1，除此之外和total_litres_of_pure_alcohol特征也非常大。得到这些关系数据后，我们就可以有针对性对这些更重要特征进行如下处理。
2.3特征分布
这部分主要是为了观察特征自己以及特征之间的更加分布关系，例如线性单增的散点分布，在那个部分的分布更加密集等，除此之外，还容易找出异常值，之后将其删除，保证数据的准确性。定义dig_dist函数，在里面作对角线网格图描述数值型特征的之间的分布（网格图中包含柱状分布图，散点分布图和密度图），相关代码如下：
g = sns.PairGrid(df)  # 主对角线是数据集中每一个数值型数据列的直方图，
g.map_diag(sns.distplot)  # 指定对角线绘图类型
g.map_upper(plt.scatter, edgecolor="white")  # 两两关系分布
g.map_lower(sns.kdeplot)  # 绘制核密度分布图。
最后用plt显示效果如下：


特征之间分布图
 
2.4缺省值统计和补全
该部分也是数据预处理非常重要的部分，尤其是数据量较少，可以保证数据完整性。首先来统计每种属性的缺省的数量df.isnull().sum()，以及缺省数占自己总数的比率，将这两个特征组合在一起构成一个新的dataframe数据来返回。
然后我们根据所有缺省之中每种特征的占比做出饼状图来显示，最后利用水平柱状图显示每种特征的缺省数量和不缺省的数量。还是使用上面的文件例子，效果如下：


每个特征的缺省统计
 

从上面可以看出continent特征有23个缺省值，占总数的0.119171，而其他特征都没有缺省。


 


 
从上面的饼状图和柱状图中可以看出所以缺省值中属于continent特征占100%，即只有continent有缺省，其他特征没有。
在知道那些特征有缺省之后就可以开始有针对性进行特征值的补全。填充的方法主要有三种：字符型特征用None填充；数值型用众数填充；通过数据拟合来填充。其中第三种的填充效果最好，但我们这里比较通用，不是特别对某个数据集处理，后面考虑改进。所以主要是使用前两种方式：
    for index in nulldata.index:
       if type(index) is not object:
           f[index].fillna("None", inplace=True)
       else:
           df[index].fillna(df[index].mode()[0], inplace=True)
 
2.5特征偏值统计和改正
现实世界中数据的分布都是服从正态分布，但是由于我们加载的csv文件中的数据往往较少，所以其分布可能会偏离正态正态分布，这就使得数据的客观性降低，对之后的使用有所影响，所有我们需要计算每种数值型特征的偏值df.skew()，将其倒排序后返回，并利用柱状图将其表示出来，效果如下：


 
对于这些偏值过高的特征进行 np.log1p()操作，降低其偏差值，这里我选择偏值大于0.5的特征进行log1p，当然具体门阀值可以按照特定的数据要求和自己需求而定。与上一个功能补全缺省值相同，我们都要都数据集进行更新，保证以后的功能都是在这个处理过的数据集上实现的。
2.6特征值的词云
以上的大部分数据分析处理都是对数值型特征进行的，而这个词云是对字符型特征的值进行统计，将特征的值在某个形态中全部显示出来，其中字体大小表示其在特征值中出现的频率。该部分主要是使用WordCloud库，将对象类型的特征的全部取出来连成一个字符串类型变量，该字符串作为作为词云的文本的输入，其中还包括背景图片设置，字体设置，停用词设置等。dinks.cvs其中的country特征的词云如下所示：

 
 
3、总结改进
这个部分的初衷是实现一个通用的自动分析和处理数据并返回结果给用户，但其中还是存在一些不足，例如增加可以处理的数据的类型，比如json数据，data数据等；在异常值发现方面，可以使用lasson回归训练数据来找到异常值并删除；除此之外，还可以做一些特征工程，如将字符型特征转化为数值型，数据向量化，增加新特征等，本来想在此基础上针对数据集实现机器学习的回归预测功能，但是由于不清楚对获取的数据集的那个特征进行预测，而且这种通用的处理的误差必然会比较大。


